# Sistema de EvaluaciÃ³n de Agentes PyCiudad

Un framework comprehensivo y organizado para evaluar, analizar y optimizar el rendimiento de agentes LLM en tareas de geolocalizaciÃ³n.

## ğŸ—ï¸ Estructura del Sistema

```
evaluation/
â”œâ”€â”€ ğŸ“ core/                    # Sistema principal de evaluaciÃ³n
â”‚   â”œâ”€â”€ agent_evaluation_system.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“ analysis/                # AnÃ¡lisis y comparaciÃ³n de mÃ©tricas  
â”‚   â”œâ”€â”€ metrics_analyzer.py
â”‚   â”œâ”€â”€ compare_model_configs.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“ configs/                 # ConfiguraciÃ³n de modelos y precios
â”‚   â”œâ”€â”€ config_loader.py
â”‚   â”œâ”€â”€ model_pricing.json
â”‚   â”œâ”€â”€ update_model_config.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“ scripts/                 # Scripts de ejecuciÃ³n
â”‚   â”œâ”€â”€ run_evaluation.py
â”‚   â”œâ”€â”€ example_evaluation.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“ utils/                   # Utilidades y herramientas
â”‚   â”œâ”€â”€ show_thinking_capabilities.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“ results/                 # Resultados de evaluaciones
â”œâ”€â”€ ğŸ“ reports/                 # Reportes generados
â””â”€â”€ requirements.txt
```

## ğŸš€ Scripts Principales (RaÃ­z del Proyecto)

### EvaluaciÃ³n
```bash
# Evaluar agentes con configuraciÃ³n actual
python run_evaluation.py --samples 50 --concurrent 2

# Evaluar todo el dataset
python run_evaluation.py --full-dataset

# Solo analizar resultados existentes
python run_evaluation.py --analyze-only
```

### AnÃ¡lisis
```bash
# Analizar resultados mÃ¡s recientes
python analyze_results.py

# ComparaciÃ³n comprehensiva de configuraciones
python compare_model_configs.py --comprehensive

# Comparar configuraciones especÃ­ficas
python compare_model_configs.py --config-names "baseline" "optimized"
```

## ğŸ“Š MÃ³dulos del Sistema

### ğŸ”§ Core (`evaluation.core`)
Sistema principal de evaluaciÃ³n que conecta con LangGraph y ejecuta agentes contra datasets.

**Clase principal**: `AgentEvaluationSystem`

```python
from evaluation import AgentEvaluationSystem

# Crear evaluador
evaluator = AgentEvaluationSystem(
    langgraph_url="http://127.0.0.1:2024",
    dataset_path="data/datasets/dataset_generado_final.json"
)

# Ejecutar evaluaciÃ³n
results = await evaluator.evaluate_all_agents(max_samples=100)
```

### ğŸ“ˆ Analysis (`evaluation.analysis`)
Herramientas avanzadas para anÃ¡lisis de mÃ©tricas y comparaciÃ³n de configuraciones.

**Clases principales**: 
- `MetricsAnalyzer`: AnÃ¡lisis detallado de una evaluaciÃ³n
- `ComprehensiveModelComparator`: ComparaciÃ³n entre mÃºltiples configuraciones

```python
from evaluation import MetricsAnalyzer, ComprehensiveModelComparator

# AnÃ¡lisis de mÃ©tricas
analyzer = MetricsAnalyzer("evaluation/results/evaluation_results_20241220_143022.json")
analyzer.load_results()
report = analyzer.generate_performance_report()

# ComparaciÃ³n de configuraciones
comparator = ComprehensiveModelComparator()
comparator.discover_all_evaluations()
comprehensive_report = comparator.generate_comprehensive_report()
```

### âš™ï¸ Configs (`evaluation.configs`)
GestiÃ³n de configuraciones de modelos, precios y parÃ¡metros.

```python
from evaluation import get_agent_model_config, get_model_pricing

# Obtener configuraciÃ³n de agentes
config = get_agent_model_config()

# Obtener precios de modelos
pricing = get_model_pricing()
```

### ğŸ”§ Utils (`evaluation.utils`)
Utilidades auxiliares y herramientas de desarrollo.

## ğŸ“‹ Flujo de Trabajo TÃ­pico

### 1. Configurar Modelos
```bash
# Configurar modelos de Ollama
export OPENAI_MODEL_NAME="qwen3:30b-a3b"
export ANTHROPIC_MODEL_NAME="qwq:latest"
export OPENAI_BASE_URL="http://localhost:11434/v1"
export ANTHROPIC_BASE_URL="http://localhost:11434/v1"
export OPENAI_API_KEY="ollama"
export ANTHROPIC_API_KEY="ollama"
```

### 2. Ejecutar EvaluaciÃ³n
```bash
python run_evaluation.py --samples 50 --concurrent 2
```

### 3. Analizar Resultados
```bash
python analyze_results.py
```

### 4. Comparar Configuraciones
```bash
# Cambiar modelos y repetir evaluaciÃ³n
export OPENAI_MODEL_NAME="phi4:14b-q8_0"
export ANTHROPIC_MODEL_NAME="phi4-reasoning:14b-plus-q8_0"
python run_evaluation.py --samples 50

# Comparar todas las configuraciones
python compare_model_configs.py --comprehensive
```

## ğŸ“ˆ Outputs del Sistema

### EvaluaciÃ³n Individual
- `evaluation/results/evaluation_results_TIMESTAMP.json`: Resultados completos
- `evaluation/performance_report.json`: Reporte de rendimiento
- `evaluation/detailed_results.csv`: Datos detallados en CSV
- `evaluation/plots/`: Visualizaciones

### ComparaciÃ³n de Configuraciones
- `evaluation/comprehensive_analysis/comprehensive_analysis_report_TIMESTAMP.json`: Reporte completo
- `evaluation/comprehensive_analysis/complete_analysis_data_TIMESTAMP.csv`: Datos completos
- `evaluation/comprehensive_analysis/plots/`: Visualizaciones avanzadas
  - `metrics_heatmap.png`: Matriz de comparaciÃ³n
  - `radar_comparison.png`: GrÃ¡fico radar
  - `rankings_by_metric.png`: Rankings
  - `efficiency_analysis.png`: AnÃ¡lisis de eficiencia
  - `temporal_evolution.png`: EvoluciÃ³n temporal

## ğŸ”§ ConfiguraciÃ³n

### Modelos Soportados
El sistema estÃ¡ optimizado para **Ollama** (modelos locales), pero tambiÃ©n soporta APIs externas:

**Modelos Ollama disponibles en tu sistema:**
- **Qwen**: `qwen3:30b-a3b` (modelo MOE eficiente)
- **QwQ (Reasoning)**: `qwq:latest` (modelo de razonamiento de 32B)
- **Phi4**: `phi4:14b-q8_0` (modelo base de Microsoft)
- **Phi4 Reasoning**: `phi4-reasoning:14b-plus-q8_0` (modelo de razonamiento)
- **Gemma3**: `gemma3:27b-it-qat` (modelo de razonamiento de Google)

**APIs externas (opcional):**
- **OpenAI**: GPT-4, GPT-3.5, etc.
- **Anthropic**: Claude series
- **Google**: Gemini series  
- **Groq**: Llama, Mixtral, etc.

### Variables de Entorno
```bash
# ConfiguraciÃ³n para Ollama (modelos locales)
# Modelo regular para todos los agentes
OLLAMA_MODEL="qwen3:30b-a3b"
OLLAMA_MODEL_THINKING="qwq:latest"
```

### Agentes Evaluados
- `agent_base`: Agente bÃ¡sico de geolocalizaciÃ³n
- `agent_intention`: Con anÃ¡lisis de intenciÃ³n
- `agent_validation`: Con validaciÃ³n de resultados
- `agent_ensemble`: Conjunto de agentes

## ğŸ“Š MÃ©tricas Principales

### Rendimiento
- **Tasa de Ã©xito**: Porcentaje de ejecuciones exitosas
- **Tiempo de ejecuciÃ³n**: EstadÃ­sticas de tiempo por query
- **Throughput**: Muestras procesadas por segundo

### Costos
- **Tokens utilizados**: Input, output y total
- **Costos estimados**: Por query y total
- **Eficiencia**: Ratio Ã©xito/costo

### Calidad
- **PrecisiÃ³n geogrÃ¡fica**: Exactitud de geolocalizaciÃ³n
- **Consistencia**: Variabilidad entre ejecuciones
- **Robustez**: Manejo de casos edge

## ğŸš€ Extensibilidad

El sistema estÃ¡ diseÃ±ado para ser extensible:

### Agregar Nuevos Agentes
1. Crear el agente en `agents/`
2. Agregarlo a `langgraph.json`
3. Incluirlo en `AgentEvaluationSystem.agents`

### Agregar Nuevas MÃ©tricas
1. Extender `MetricsAnalyzer._extract_token_metrics()`
2. Actualizar visualizaciones en `create_visualizations()`

### Agregar Nuevos Modelos
1. Actualizar `evaluation/configs/model_pricing.json`
2. Configurar variables de entorno correspondientes

## ğŸ› ï¸ Desarrollo

### Requisitos Previos
```bash
# 1. Ollama debe estar ejecutÃ¡ndose
ollama serve

# 2. Descargar modelos necesarios
ollama pull qwen3:30b-a3b
ollama pull qwq:latest
ollama pull phi4:14b-q8_0
ollama pull phi4-reasoning:14b-plus-q8_0
ollama pull gemma3:27b-it-qat

# 3. Verificar modelos disponibles
ollama list
```

### InstalaciÃ³n
```bash
pip install -r requirements.txt
```

### Verificar ConfiguraciÃ³n de Ollama
```bash
# Verificar que Ollama estÃ© funcionando
curl http://localhost:11434/api/tags

# Probar un modelo especÃ­fico
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3:30b-a3b",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

### Testing
```bash
# EvaluaciÃ³n rÃ¡pida de prueba
python run_evaluation.py --samples 5 --concurrent 1

# Verificar configuraciÃ³n
python -c "from evaluation import get_agent_model_config; print(get_agent_model_config())"
```

### Debugging
```bash
# Mostrar capacidades de thinking
python evaluation/utils/show_thinking_capabilities.py

# Verificar configuraciÃ³n de modelos
python evaluation/configs/update_model_config.py

# Verificar conectividad con Ollama
python -c "
import requests
try:
    r = requests.get('http://localhost:11434/api/tags')
    print('âœ… Ollama conectado:', r.status_code)
    print('Modelos disponibles:', [m['name'] for m in r.json()['models']])
except:
    print('âŒ Error conectando con Ollama')
"
```


## ğŸ“ Licencia

Ver `LICENSE` en la raÃ­z del proyecto.