#!/usr/bin/env python3
"""
Script Principal de Evaluaci√≥n de Agentes

Este script ejecuta el sistema completo de evaluaci√≥n de agentes,
incluyendo la ejecuci√≥n contra el dataset y el an√°lisis de m√©tricas.

Uso:
    python run_evaluation.py --samples 50 --concurrent 2
    python run_evaluation.py --full-dataset
    python run_evaluation.py --analyze-only
"""

import argparse
import asyncio
import sys
from pathlib import Path
import json
import numpy as np

# Agregar el directorio ra√≠z al path para importar m√≥dulos
sys.path.append(str(Path(__file__).parent.parent.parent))

from evaluation.core.agent_evaluation_system import AgentEvaluationSystem
# from evaluation.analysis.metrics_analyzer import MetricsAnalyzer  # TODO: Implementar


def numpy_json_serializer(obj):
    """Serializador personalizado para manejar tipos numpy"""
    if hasattr(obj, 'dtype'):
        if np.issubdtype(obj.dtype, np.integer):
            return int(obj)
        elif np.issubdtype(obj.dtype, np.floating):
            return float(obj)
        elif np.issubdtype(obj.dtype, np.bool_):
            return bool(obj)
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    # Fallback para tipos espec√≠ficos de numpy
    if str(type(obj)).startswith('<class \'numpy.'):
        if 'int' in str(type(obj)):
            return int(obj)
        elif 'float' in str(type(obj)):
            return float(obj)
        elif 'bool' in str(type(obj)):
            return bool(obj)
    # Manejar m√©todos y funciones (no serializar)
    if callable(obj):
        return f"<callable: {str(obj)}>"
    # Manejar sets
    if isinstance(obj, set):
        return list(obj)
    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")


def parse_arguments():
    """Configurar argumentos de l√≠nea de comandos"""
    parser = argparse.ArgumentParser(
        description="Sistema de Evaluaci√≥n de Agentes PyCiudad",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  # Evaluar 50 muestras con 2 ejecuciones concurrentes
  python run_evaluation.py --samples 50 --concurrent 2
  
  # Evaluar todo el dataset (puede tomar mucho tiempo)
  python run_evaluation.py --full-dataset
  
  # Solo analizar resultados existentes
  python run_evaluation.py --analyze-only
  
  # Configurar URL personalizada de LangGraph
  python run_evaluation.py --url http://localhost:2024 --samples 20
        """
    )
    
    # Argumentos principales
    parser.add_argument(
        "--samples",
        type=int,
        default=100,
        help="N√∫mero m√°ximo de muestras del dataset a evaluar (default: 100)"
    )
    
    parser.add_argument(
        "--full-dataset",
        action="store_true",
        help="Evaluar todo el dataset (ignora --samples)"
    )
    
    parser.add_argument(
        "--concurrent",
        type=int,
        default=3,
        help="N√∫mero m√°ximo de ejecuciones concurrentes por agente (default: 3)"
    )
    
    parser.add_argument(
        "--url",
        type=str,
        default="http://127.0.0.1:2024",
        help="URL del servicio LangGraph (default: http://127.0.0.1:2024)"
    )
    
    parser.add_argument(
        "--dataset",
        type=str,
        default="data/datasets/dataset_generado_final.json",
        help="Ruta al archivo del dataset (default: data/datasets/dataset_generado_final.json)"
    )
    
    # Argumentos de control
    parser.add_argument(
        "--analyze-only",
        action="store_true",
        help="Solo analizar resultados existentes, no ejecutar evaluaci√≥n"
    )
    
    parser.add_argument(
        "--agents",
        nargs="+",
        default=["agent_base", "agent_intention", "agent_validation", "agent_ensemble"],
        help="Lista de agentes a evaluar (default: todos)"
    )
    
    # Argumentos de salida
    parser.add_argument(
        "--output-dir",
        type=str,
        default="evaluation",
        help="Directorio base para guardar resultados (default: evaluation)"
    )
    
    return parser.parse_args()


async def run_evaluation(args):
    """Ejecutar evaluaci√≥n de agentes"""
    
    print("üöÄ Iniciando evaluaci√≥n de agentes...")
    print(f"üìä Configuraci√≥n:")
    print(f"   - Muestras: {'Todo el dataset' if args.full_dataset else args.samples}")
    print(f"   - Concurrencia: {args.concurrent}")
    print(f"   - URL LangGraph: {args.url}")
    print(f"   - Agentes: {', '.join(args.agents)}")
    
    # Verificar que el dataset existe
    if not Path(args.dataset).exists():
        print(f"‚ùå Dataset no encontrado: {args.dataset}")
        return False
    
    # Crear evaluador
    evaluator = AgentEvaluationSystem(
        langgraph_url=args.url,
        dataset_path=args.dataset
    )
    
    # Configurar agentes a evaluar
    evaluator.agents = args.agents
    
    try:
        # Ejecutar evaluaci√≥n
        max_samples = None if args.full_dataset else args.samples
        results = await evaluator.evaluate_all_agents(
            max_samples=max_samples,
            max_concurrent=args.concurrent
        )
        
        # Guardar resultados
        output_path = evaluator.save_results(results)
        
        print(f"\n‚úÖ Evaluaci√≥n completada exitosamente")
        print(f"üíæ Resultados guardados en: {output_path}")
        
        return output_path
        
    except Exception as e:
        print(f"‚ùå Error durante la evaluaci√≥n: {e}")
        return False


def run_analysis(args, results_file=None):
    """Ejecutar an√°lisis de m√©tricas"""
    
    print("\nüìä Iniciando an√°lisis de m√©tricas...")
    print("‚ö†Ô∏è  An√°lisis de m√©tricas no disponible - MetricsAnalyzer no implementado")
    return True
    
    # TODO: Implementar MetricsAnalyzer
    # try:
    #     # Crear analizador
    #     analyzer = MetricsAnalyzer(results_file)
    #     
    #     if results_file:
    #         analyzer.load_results()
    #     else:
    #         # Buscar el archivo m√°s reciente
    #         results_dir = Path(args.output_dir) / "results"
    #         if not results_dir.exists():
    #             print("‚ùå No se encontraron resultados para analizar")
    #             return False
    #         
    #         # Buscar archivos con el nuevo formato primero (samples)
    #         result_files = list(results_dir.glob("results_*samples_*.json"))
    #         if not result_files:
    #             # Buscar archivos con el formato anterior (candidatos)
    #             result_files = list(results_dir.glob("results_*cand_*.json"))
    #         if not result_files:
    #             # Buscar archivos con formato muy anterior
    #             result_files = list(results_dir.glob("evaluation_results_*.json"))
    #             
    #         if not result_files:
    #             print("‚ùå No se encontraron archivos de resultados")
    #             return False
    #             
    #         latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
    #         print(f"üìÇ Usando archivo: {latest_file}")
    #         analyzer.load_results(str(latest_file))
    #     
    #     # Crear DataFrame
    #     df = analyzer.create_dataframe()
    #     print(f"üìÑ Datos procesados: {len(df)} filas")
    #     
    #     # Generar reporte
    #     report = analyzer.generate_performance_report()
    #     
    #     # Guardar reporte con nombre descriptivo
    #     base_name = analyzer._generate_base_filename()
    #     report_path = Path(args.output_dir) / f"performance_report_{base_name}.json"
    #     report_path.parent.mkdir(parents=True, exist_ok=True)
    #     
    #     with open(report_path, 'w', encoding='utf-8') as f:
    #         json.dump(report, f, indent=2, ensure_ascii=False, default=numpy_json_serializer)
    #     print(f"üìã Reporte guardado en: {report_path}")
    #     
    #     # Exportar CSV detallado con nombre descriptivo
    #     analyzer.export_detailed_csv()
    #     
    #     # Mostrar resumen
    #     print_summary(report, analyzer.compare_agents())
    #     
    #     return True
    #     
    # except Exception as e:
    #     print(f"‚ùå Error durante el an√°lisis: {e}")
    #     return False


def print_summary(report, comparison_df):
    """Imprimir resumen de resultados con m√©tricas de calidad"""
    
    print("\n" + "="*120)
    print("üìä RESUMEN DE EVALUACI√ìN CON GROUND TRUTH")
    print("="*120)
    
    # Estad√≠sticas generales
    general = report["general_statistics"]
    print(f"üìà Estad√≠sticas Generales:")
    print(f"   ‚Ä¢ Total de ejecuciones: {general['total_executions']}")
    print(f"   ‚Ä¢ Agentes evaluados: {general['total_agents']}")
    print(f"   ‚Ä¢ Tiempo total: {general['total_execution_time']:.2f} segundos")
    print(f"   ‚Ä¢ Costo total estimado: ${general['total_estimated_cost']:.4f}")
    
    if general['total_executions'] > 0:
        print(f"   ‚Ä¢ Costo promedio por query: ${general['total_estimated_cost']/general['total_executions']:.4f}")
    
    # NUEVAS m√©tricas de calidad
    print(f"\nüéØ M√âTRICAS DE CALIDAD (Ground Truth):")
    print(f"   ‚Ä¢ Score de calidad promedio: {general['average_quality_score']:.3f}")
    print(f"   ‚Ä¢ Encontrado en resultados: {general['found_in_results_rate']:.2%}")
    print(f"   ‚Ä¢ Perfect hits (posici√≥n 1): {general['perfect_rate']:.2%}")
    print(f"   ‚Ä¢ Top 3 hits: {general['top_3_rate']:.2%}")
    print(f"   ‚Ä¢ Top 5 hits: {general['top_5_rate']:.2%}")
    
    # NUEVAS m√©tricas combinadas
    print(f"\n‚öñÔ∏è  M√âTRICAS COMBINADAS:")
    print(f"   ‚Ä¢ √âxito t√©cnico √∫nicamente: {general['technical_success_rate']:.2%}")
    print(f"   ‚Ä¢ √âxito combinado (t√©cnico + calidad): {general['combined_success_rate']:.2%}")
    print(f"   ‚Ä¢ Nueva tasa de √©xito: {general['overall_success_rate']:.2%}")
    
    # Distribuci√≥n de tiers de calidad
    if 'quality_distribution' in general:
        print(f"\nüìä DISTRIBUCI√ìN DE CALIDAD:")
        quality_dist = general['quality_distribution']
        total_samples = sum(quality_dist.values())
        for tier, count in quality_dist.items():
            percentage = count / total_samples * 100 if total_samples > 0 else 0
            tier_names = {
                'perfect': 'Perfecto (pos. 1)',
                'top_3': 'Top 3',
                'top_5': 'Top 5', 
                'found_far': 'Encontrado (pos. >5)',
                'not_found': 'No encontrado',
                'error': 'Error t√©cnico',
                'unknown': 'Desconocido'
            }
            tier_display = tier_names.get(tier, tier)
            print(f"   ‚Ä¢ {tier_display}: {count} ({percentage:.1f}%)")
    
    # Comparaci√≥n de agentes actualizada
    print(f"\nü§ñ COMPARACI√ìN DE AGENTES (Con M√©tricas de Calidad):")
    print("-" * 120)
    
    for _, row in comparison_df.iterrows():
        agent = row['agent_name']
        print(f"\n{agent.upper()}:")
        
        # M√©tricas b√°sicas
        print(f"   üìä B√°sico: {row.get('total_executions', 0):.0f} ejecuciones")
        
        # M√©tricas t√©cnicas vs combinadas
        tech_success = row.get('technical_success_rate', 0)
        combined_success = row.get('combined_success_rate', row.get('success_rate', 0))
        print(f"   ‚úÖ √âxito: T√©cnico {tech_success:.2%} | Combinado {combined_success:.2%}")
        
        # M√©tricas de calidad
        quality_score = row.get('average_quality_score', 0)
        perfect_rate = row.get('perfect_rate', 0)
        top_3_rate = row.get('top_3_rate', 0)
        print(f"   üéØ Calidad: Score {quality_score:.3f} | Perfect {perfect_rate:.2%} | Top-3 {top_3_rate:.2%}")
        
        # M√©tricas de rendimiento
        avg_time = row.get('avg_execution_time', 0)
        std_time = row.get('std_execution_time', 0)
        print(f"   ‚è±Ô∏è  Tiempo: {avg_time:.2f}s promedio (¬±{std_time:.2f}s)")
        
        # M√©tricas de costo
        total_cost = row.get('total_cost', 0)
        avg_cost = row.get('avg_cost', 0)
        print(f"   üí∞ Costo: ${total_cost:.4f} total (${avg_cost:.4f} promedio)")
        
        # Tokens
        if 'total_tokens' in row:
            total_tokens = row.get('total_tokens', 0)
            avg_tokens = row.get('avg_tokens', 0)
            print(f"   üî§ Tokens: {total_tokens:.0f} total ({avg_tokens:.0f} promedio)")
    
    # An√°lisis thinking vs regular actualizado
    if 'thinking_vs_regular_analysis' in report:
        thinking_analysis = report["thinking_vs_regular_analysis"]
        print(f"\nüß† COMPARACI√ìN THINKING VS REGULAR (Con Calidad):")
        
        thinking_stats = thinking_analysis['agents_with_thinking']
        regular_stats = thinking_analysis['agents_without_thinking']
        
        # Validar que hay datos para mostrar
        if thinking_stats['count'] > 0 or regular_stats['count'] > 0:
            print(f"Agentes con thinking: {thinking_stats['count']}")
            if thinking_stats['count'] > 0:
                print(f"  ‚Ä¢ √âxito t√©cnico: {thinking_stats.get('technical_success_rate', 0):.2%}")
                print(f"  ‚Ä¢ Score de calidad: {thinking_stats.get('average_quality_score', 0):.3f}")
                print(f"  ‚Ä¢ √âxito combinado: {thinking_stats.get('combined_success_rate', 0):.2%}")
                print(f"  ‚Ä¢ Costo promedio: ${thinking_stats.get('avg_cost_per_query', 0):.4f}")
                print(f"  ‚Ä¢ Tiempo promedio: {thinking_stats.get('avg_execution_time', 0):.2f}s")
            else:
                print(f"  ‚Ä¢ No hay datos disponibles")
            
            print(f"Agentes sin thinking: {regular_stats['count']}")
            if regular_stats['count'] > 0:
                print(f"  ‚Ä¢ √âxito t√©cnico: {regular_stats.get('technical_success_rate', 0):.2%}")
                print(f"  ‚Ä¢ Score de calidad: {regular_stats.get('average_quality_score', 0):.3f}")
                print(f"  ‚Ä¢ √âxito combinado: {regular_stats.get('combined_success_rate', 0):.2%}")
                print(f"  ‚Ä¢ Costo promedio: ${regular_stats.get('avg_cost_per_query', 0):.4f}")
                print(f"  ‚Ä¢ Tiempo promedio: {regular_stats.get('avg_execution_time', 0):.2f}s")
            else:
                print(f"  ‚Ä¢ No hay datos disponibles")
            
            # Calcular mejoras solo si ambos tienen datos
            if thinking_stats['count'] > 0 and regular_stats['count'] > 0:
                quality_improvement = thinking_stats.get('average_quality_score', 0) - regular_stats.get('average_quality_score', 0)
                combined_improvement = thinking_stats.get('combined_success_rate', 0) - regular_stats.get('combined_success_rate', 0)
                cost_increase = thinking_stats.get('avg_cost_per_query', 0) - regular_stats.get('avg_cost_per_query', 0)
                
                print(f"\nüí° AN√ÅLISIS DE ROI (Thinking Models):")
                print(f"   ‚Ä¢ Mejora en calidad: {quality_improvement:+.3f}")
                print(f"   ‚Ä¢ Mejora en √©xito combinado: {combined_improvement:+.2%}")
                print(f"   ‚Ä¢ Incremento en costo: ${cost_increase:+.4f}")
                
                if cost_increase > 0:
                    roi_quality = quality_improvement / cost_increase
                    roi_success = (combined_improvement * 100) / cost_increase
                    print(f"   ‚Ä¢ ROI calidad: {roi_quality:.2f} puntos por $")
                    print(f"   ‚Ä¢ ROI √©xito: {roi_success:.2f} pp de √©xito por $")
        else:
            print(f"  ‚Ä¢ No hay datos suficientes para comparaci√≥n")
    
    # An√°lisis por dificultad si est√° disponible
    if 'difficulty_level_analysis' in report and report['difficulty_level_analysis']:
        print(f"\nüìà AN√ÅLISIS POR NIVEL DE DIFICULTAD:")
        difficulty_analysis = report['difficulty_level_analysis']
        
        for difficulty, stats in difficulty_analysis.items():
            print(f"  {difficulty.upper()}:")
            print(f"    ‚Ä¢ Muestras: {stats['total_samples']}")
            print(f"    ‚Ä¢ √âxito combinado: {stats['combined_success_rate']:.2%}")
            print(f"    ‚Ä¢ Score calidad: {stats['average_quality_score']:.3f}")
            print(f"    ‚Ä¢ Perfect rate: {stats['perfect_rate']:.2%}")
    
    print(f"\nüìã NUEVA DEFINICI√ìN DE √âXITO:")
    print(f"   ‚úÖ √âxito = Ejecuci√≥n t√©cnica exitosa AND Score de calidad > 0.0")
    print(f"   üìä Sistema de scoring por posici√≥n:")
    print(f"      ‚Ä¢ Posici√≥n 1: 1.0 puntos")
    print(f"      ‚Ä¢ Posiciones 1-3: 0.8 puntos") 
    print(f"      ‚Ä¢ Posiciones 1-5: 0.6 puntos")
    print(f"      ‚Ä¢ M√°s de 5 posiciones: 0.3 puntos")
    print(f"      ‚Ä¢ No encontrado: 0.0 puntos")
    
    print("\n" + "="*120)


def main():
    """Funci√≥n principal"""
    
    args = parse_arguments()
    
    print("üéØ Sistema de Evaluaci√≥n de Agentes PyCiudad")
    print("=" * 50)
    
    success = True
    results_file = None
    
    # Ejecutar evaluaci√≥n si no es solo an√°lisis
    if not args.analyze_only:
        results_file = asyncio.run(run_evaluation(args))
        if not results_file:
            success = False
    
    # Ejecutar an√°lisis
    if success:
        analysis_success = run_analysis(args, results_file)
        if not analysis_success:
            success = False
    
    # Resultado final
    if success:
        print("\nüéâ Proceso completado exitosamente")
        print(f"üìÅ Resultados en: {args.output_dir}/")
    else:
        print("\n‚ùå Proceso terminado con errores")
        sys.exit(1)


if __name__ == "__main__":
    main() 